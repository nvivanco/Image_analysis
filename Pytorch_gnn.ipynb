{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eba3739-07bd-42e5-bf06-61344a56da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b34e181-46b5-4100-aafc-04a7b4f2e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import find_contours\n",
    "from skimage.draw import polygon\n",
    "from matplotlib.collections import LineCollection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f815bae-f423-4fb5-914b-cb8367df4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'DUMM_giTG69_Glucose_013025'\n",
    "all_cells_filename = f'/Users/noravivancogonzalez/Documents/DuMM_image_analysis/all_cell_data_{folder}.pkl'\n",
    "all_cells_pd = pd.read_pickle(all_cells_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d54763-0f4d-431e-b487-0db1537a155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV = '007'\n",
    "trench_id = '295'\n",
    "df = all_cells_pd[(all_cells_pd['FOV'] == FOV) & (all_cells_pd['trench_id'] == trench_id)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e7e0c0-e9d8-4e0e-9907-05dc8fe1d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['track_id'] = df['track_id'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4361704a-0196-42eb-92f3-0960e09e3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fov 007 and trench id 295\n",
    "# manually correcting lineages id from track id\n",
    "ground_truth_lineage_id_dict = {'7': 'A',\n",
    "                                '25':'A.1',\n",
    "                                '46':'A.1',\n",
    "                                '67':'A.1',\n",
    "                                '73':'A.1.1',\n",
    "                                '79':'A.1.1',\n",
    "                                '97':'A.1.1',\n",
    "                                '102':'A.1.1',\n",
    "                                '108':'A.1.1',\n",
    "                                '115':'A.1.1',\n",
    "                                '119':'A.1.1',\n",
    "                                '124':'A.1.1',\n",
    "                                '130':'A.1.1',\n",
    "                                '132':'A.1.1',\n",
    "                                '135':'A.1.1',\n",
    "                                '140':'A.1.1',\n",
    "                                '142':'A.1.1',\n",
    "                                '143':'A.1.1.1',\n",
    "                                '151':'A.1.1.1',\n",
    "                                '154':'A.1.1.1',\n",
    "                                '159':'A.1.1.1',\n",
    "                                '163':'A.1.1.1',\n",
    "                                '167':'A.1.1.1',\n",
    "                                '172':'A.1.1.1',\n",
    "                                '178':'A.1.1.1',\n",
    "                                '183':'A.1.1.1',\n",
    "                                '144':'A.1.1.2',\n",
    "                                '152':'A.1.1.2',\n",
    "                                '155':'A.1.1.2',\n",
    "                                '160':'A.1.1.2',\n",
    "                                '164':'A.1.1.2',\n",
    "                                '168':'A.1.1.2',\n",
    "                                '173':'A.1.1.2',\n",
    "                                '179':'A.1.1.2',\n",
    "                                '184':'A.1.1.2',\n",
    "                                '187':'A.1.1.2',\n",
    "                                '191':'A.1.1.2',\n",
    "                                '196':'A.1.1.2',\n",
    "                                '202':'A.1.1.2',\n",
    "                                '74':'A.1.2',\n",
    "                                '80':'A.1.2',\n",
    "                                '93':'A.1.2',\n",
    "                                '109':'A.1.2',\n",
    "                                '120':'A.1.2',\n",
    "                                '125':'A.1.2',\n",
    "                                '131':'A.1.2',\n",
    "                                '133':'A.1.2',\n",
    "                                '136':'A.1.2',\n",
    "                                '141':'A.1.2',\n",
    "                                '145':'A.1.2.1',\n",
    "                                '150':'A.1.2.1',\n",
    "                                '156':'A.1.2.1',\n",
    "                                '161':'A.1.2.1',\n",
    "                                '169':'A.1.2.1',\n",
    "                                '174':'A.1.2.1',\n",
    "                                '180':'A.1.2.1',\n",
    "                                '188':'A.1.2.1',\n",
    "                                '192':'A.1.2.1',\n",
    "                                '197':'A.1.2.1',\n",
    "                                '203':'A.1.2.1',\n",
    "                                '208':'A.1.2.1',\n",
    "                                '213':'A.1.2.1',\n",
    "                                '146':'A.1.2.2',\n",
    "                                '162':'A.1.2.2',\n",
    "                                '165':'A.1.2.2',\n",
    "                                '170':'A.1.2.2',\n",
    "                                '175':'A.1.2.2',\n",
    "                                '181':'A.1.2.2',\n",
    "                                '193':'A.1.2.2',\n",
    "                                '198':'A.1.2.2',\n",
    "                                '204':'A.1.2.2',\n",
    "                                '209':'A.1.2.2',\n",
    "                                '216':'A.1.2.2',\n",
    "                                '222':'A.1.2.2',\n",
    "                                '74':'A.1.2',\n",
    "                                '19': 'A.2',\n",
    "                                '81': 'A.2.1',\n",
    "                                '88':'A.2.1',\n",
    "                                '94':'A.2.1',\n",
    "                                '137':'A.2.1',\n",
    "                                '147':'A.2.1.1',\n",
    "                                '166':'A.2.1.1',\n",
    "                                '176':'A.2.1.1',\n",
    "                                '182':'A.2.1.1',\n",
    "                                '189':'A.2.1.1',\n",
    "                                '194':'A.2.1.1',\n",
    "                                '199':'A.2.1.1',\n",
    "                                '205':'A.2.1.1',\n",
    "                                '210':'A.2.1.1',\n",
    "                                '217':'A.2.1.1',\n",
    "                                '223':'A.2.1.1',\n",
    "                                '229':'A.2.1.1',\n",
    "                                '234':'A.2.1.1',\n",
    "                                '148':'A.2.1.2',\n",
    "                                '157':'A.2.1.2',\n",
    "                                '177':'A.2.1.2',\n",
    "                                '195':'A.2.1.2',\n",
    "                                '206':'A.2.1.2',\n",
    "                                '211':'A.2.1.2',\n",
    "                                '214':'A.2.1.2',\n",
    "                                '218':'A.2.1.2',\n",
    "                                '224':'A.2.1.2.1',\n",
    "                                '230':'A.2.1.2.1',\n",
    "                                '235':'A.2.1.2.1',\n",
    "                                '240':'A.2.1.2.1',\n",
    "                                '245':'A.2.1.2.1',\n",
    "                                '249':'A.2.1.2.1',\n",
    "                                '225':'A.2.1.2.2',\n",
    "                                '82': 'A.2.2',\n",
    "                                '138':'A.2.2.1',\n",
    "                                '158':'A.2.2.1',\n",
    "                                '171':'A.2.2.1',\n",
    "                                '190':'A.2.2.1',\n",
    "                                '207':'A.2.2.1',\n",
    "                                '212':'A.2.2.1',\n",
    "                                '220':'A.2.2.1',\n",
    "                                '227':'A.2.2.1',\n",
    "                                '232':'A.2.2.1',\n",
    "                                '238':'A.2.2.1',\n",
    "                                '243':'A.2.2.1',\n",
    "                                '247':'A.2.2.1',\n",
    "                                '255':'A.2.2.1',\n",
    "                                '258':'A.2.2.1',\n",
    "                                '262':'A.2.2.1',\n",
    "                                '264':'A.2.2.1',\n",
    "                                '267':'A.2.2.1.1',\n",
    "                                '268':'A.2.2.1.2',\n",
    "                                '139':'A.2.2.2',\n",
    "                                '200': 'A.2.2.2.1',\n",
    "                                '215': 'A.2.2.2.1',\n",
    "                                '221': 'A.2.2.2.1',\n",
    "                                '228': 'A.2.2.2.1',\n",
    "                                '233': 'A.2.2.2.1',\n",
    "                                '239': 'A.2.2.2.1',\n",
    "                                '244': 'A.2.2.2.1',\n",
    "                                '248': 'A.2.2.2.1',\n",
    "                                '252': 'A.2.2.2.1',\n",
    "                                '256': 'A.2.2.2.1',\n",
    "                                '259': 'A.2.2.2.1',\n",
    "                                '263': 'A.2.2.2.1',\n",
    "                                '265': 'A.2.2.2.1',\n",
    "                                '266': 'A.2.2.2.1',\n",
    "                                '269': 'A.2.2.2.1',\n",
    "                                '272': 'A.2.2.2.1',\n",
    "                                '274': 'A.2.2.2.1',\n",
    "                                '201': 'A.2.2.2.2',\n",
    "                                '273': 'A.2.2.2.2',\n",
    "                                '277': 'A.2.2.2.2',\n",
    "                                '278': 'A.2.2.2.2.1',\n",
    "                                '280': 'A.2.2.2.2.1',\n",
    "                                '281': 'A.2.2.2.2.1',\n",
    "                                '282': 'A.2.2.2.2.1',\n",
    "                                '283': 'A.2.2.2.2.1',\n",
    "                                '284': 'A.2.2.2.2.1',\n",
    "                                '285': 'A.2.2.2.2.1',\n",
    "                                '286': 'A.2.2.2.2.1',\n",
    "                                '287': 'A.2.2.2.2.1.1',\n",
    "                                '288': 'A.2.2.2.2.1.2',\n",
    "                                '279': 'A.2.2.2.2.2',\n",
    "                                '289': 'A.2.2.2.2.1',\n",
    "                                '291': 'A.2.2.2.2.1',\n",
    "                                '292': 'A.2.2.2.2.1',\n",
    "                                '293': 'A.2.2.2.2.1',\n",
    "                                '294': 'A.2.2.2.2.1',\n",
    "                                '295': 'A.2.2.2.2.1',\n",
    "                                '296': 'A.2.2.2.2.1',\n",
    "                                '297': 'A.2.2.2.2.1',\n",
    "                                '298': 'A.2.2.2.2.1',\n",
    "                                '290': 'A.2.2.2.2.2.2',\n",
    "                                '299': 'A.2.2.2.2.2.2',\n",
    "                                '300': 'A.2.2.2.2.2.2',\n",
    "                                '301': 'A.2.2.2.2.2.2',\n",
    "                                '302': 'A.2.2.2.2.2.2.1',\n",
    "                                '304': 'A.2.2.2.2.2.2.1',\n",
    "                                '305': 'A.2.2.2.2.2.2.1',\n",
    "                                '306': 'A.2.2.2.2.2.2.1',\n",
    "                                '307': 'A.2.2.2.2.2.2.1',\n",
    "                                '308': 'A.2.2.2.2.2.2.1',\n",
    "                                '309': 'A.2.2.2.2.2.2.1',\n",
    "                                '303': 'A.2.2.2.2.2.2.2',\n",
    "                                '310': 'A.2.2.2.2.2.2.2',\n",
    "                                '311': 'A.2.2.2.2.2.2.2',\n",
    "                                '312': 'A.2.2.2.2.2.2.2',\n",
    "                                '313': 'A.2.2.2.2.2.2.2.1',\n",
    "                                '315': 'A.2.2.2.2.2.2.2.1',\n",
    "                                '316': 'A.2.2.2.2.2.2.2.1',\n",
    "                                '317': 'A.2.2.2.2.2.2.2.1',\n",
    "                                '318': 'A.2.2.2.2.2.2.2.1',\n",
    "                                '319': 'A.2.2.2.2.2.2.2.1',\n",
    "                                '320': 'A.2.2.2.2.2.2.2.1',\n",
    "                                '314': 'A.2.2.2.2.2.2.2.2',\n",
    "                                '321': 'A.2.2.2.2.2.2.2.2',\n",
    "                                '322': 'A.2.2.2.2.2.2.2.2',\n",
    "                                '323': 'A.2.2.2.2.2.2.2.2',\n",
    "                                '324': 'A.2.2.2.2.2.2.2.2',\n",
    "                                '325': 'A.2.2.2.2.2.2.2.2',\n",
    "                                '326': 'A.2.2.2.2.2.2.2.2',\n",
    "                                '327': 'A.2.2.2.2.2.2.2.2.1',\n",
    "                                '329': 'A.2.2.2.2.2.2.2.2.1',\n",
    "                                '330': 'A.2.2.2.2.2.2.2.2.1',\n",
    "                                '328': 'A.2.2.2.2.2.2.2.2.2',\n",
    "                                '331': 'A.2.2.2.2.2.2.2.2.2',\n",
    "                                '332': 'A.2.2.2.2.2.2.2.2.2.1',\n",
    "                                '333': 'A.2.2.2.2.2.2.2.2.2.2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab0a1c8-2e72-47db-8100-92b8bd1d2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ground_truth_lineage'] = None\n",
    "df['ground_truth_lineage']= df['track_id'].map(ground_truth_lineage_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce104b1-1b43-4bf6-aed8-21f3526183b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'centroid-0': 'centroid_y','centroid-1': 'centroid_x'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d4c96f-bf4d-4c98-8e70-abe64145abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cells = df[df['ground_truth_lineage'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c15a92-a31a-425f-9c1a-c756c77be70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cells.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c157cd51-12ac-41b6-a453-e53a869e0622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'area', 'coords', 'centroid_y', 'centroid_x',\n",
       "       'axis_major_length', 'axis_minor_length', 'intensity_mean_phase',\n",
       "       'intensity_max_phase', 'intensity_min_phase', 'intensity_mean_fluor',\n",
       "       'intensity_max_fluor', 'intensity_min_fluor', 'time_frame',\n",
       "       'experiment_name', 'FOV', 'trench_id', 'track_id',\n",
       "       'ground_truth_lineage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cells.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e43c514-8d79-48ee-815e-f33c1ab576ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n",
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1998573771.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells[col] = df_cells[col].astype(np.float32);\n"
     ]
    }
   ],
   "source": [
    "# Define node features\n",
    "node_feature_cols = ['area', 'centroid_y', \n",
    "       'axis_major_length', 'axis_minor_length', 'intensity_mean_phase',\n",
    "       'intensity_max_phase', 'intensity_min_phase', 'intensity_mean_fluor',\n",
    "       'intensity_max_fluor', 'intensity_min_fluor']\n",
    "\n",
    "for col in node_feature_cols:\n",
    "    df_cells[col] = df_cells[col].astype(np.float32);               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394d176a-45d9-4319-9fc4-cb691a3dbe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lineage IDs to unique integers as labels\n",
    "all_unique_lineages = sorted(df_cells['ground_truth_lineage'].unique())\n",
    "lineage_to_int_mapping = {lineage: i for i, lineage in enumerate(all_unique_lineages)}\n",
    "num_lineage_classes = len(all_unique_lineages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12d3f717-80e9-4038-aaeb-2ef0f45526aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/1883193674.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells['numeric_lineage'] = df_cells['ground_truth_lineage'].map(lineage_to_int_mapping)\n"
     ]
    }
   ],
   "source": [
    "df_cells['numeric_lineage'] = df_cells['ground_truth_lineage'].map(lineage_to_int_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37dd8a0-16ef-4ec6-8641-a81c7b8b61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_sub_lineage_roots(df):\n",
    "    sub_lineage_roots = []\n",
    "    # Sort again locally for safety if df was not pre-sorted in this exact way\n",
    "    df_sorted = df.sort_values(by=['time_frame', 'ground_truth_lineage'])\n",
    "\n",
    "    # Track seen lineage IDs at previous time step\n",
    "    prev_time_lineages = set()\n",
    "\n",
    "    for t in sorted(df_sorted['time_frame'].unique()):\n",
    "        df_current_t = df_sorted[df_sorted['time_frame'] == t]\n",
    "        current_time_lineages = set(df_current_t['ground_truth_lineage'])\n",
    "\n",
    "        for lineage_id in current_time_lineages:\n",
    "            parent_lineage = '.'.join(lineage_id.split('.')[:-1])\n",
    "\n",
    "            is_top_level_lineage = '.' not in lineage_id and t == 0 # First appearance at t=0\n",
    "            is_daughter_lineage = parent_lineage in prev_time_lineages and lineage_id not in prev_time_lineages\n",
    "\n",
    "            # Store (experiment_name, FOV, trench_id, ground_truth_lineage, time_frame) of the first cell of that type\n",
    "            if df_current_t[df_current_t['ground_truth_lineage'] == lineage_id]['time_frame'].min() == t:\n",
    "                root_node_df = df_current_t[df_current_t['ground_truth_lineage'] == lineage_id].sort_values(by='node_id').iloc[0]\n",
    "                sub_lineage_roots.append((\n",
    "                    root_node_df['experiment_name'],\n",
    "                    root_node_df['FOV'],\n",
    "                    root_node_df['trench_id'],\n",
    "                    root_node_df['ground_truth_lineage'], # This will be the new effective root ID\n",
    "                    root_node_df['time_frame'] # And its starting time\n",
    "                ))\n",
    "        prev_time_lineages = current_time_lineages # Update for next iteration\n",
    "\n",
    "    return sub_lineage_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6feaa78-5187-4670-b2c6-5cb8f8a26357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hj/bk95lprn3zb88y7bqhjh9j5m0000gr/T/ipykernel_97404/157364811.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cells['node_id'] = df_cells.index # Assign unique global node ID\n"
     ]
    }
   ],
   "source": [
    "df_cells['node_id'] = df_cells.index # Assign unique global node ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7398422-0423-484d-9885-2a0d334e68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c66d7841-22aa-4321-bd83-e2f55bfcdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function expects a sub-DataFrame already filtered for a specific lineage branch\n",
    "def create_lineage_graph(df_lineage, device='cpu'):\n",
    "    original_global_node_ids = df_lineage['node_id'].values\n",
    "    global_id_to_local_idx = {global_id: i for i, global_id in enumerate(original_global_node_ids)}\n",
    "\n",
    "    x = torch.tensor(df_lineage[node_feature_cols].values, dtype=torch.float).to(device)\n",
    "    y = torch.tensor(df_lineage['numeric_lineage'].values, dtype=torch.long).to(device)\n",
    "    pos = torch.tensor(df_lineage['centroid_y'].values, dtype=torch.float).to(device)\n",
    "    node_time_frames = torch.tensor(df_lineage['time_frame'].values, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    num_nodes = len(df_lineage)\n",
    "    if num_nodes == 0:\n",
    "        return None\n",
    "\n",
    "    source_nodes_local_idx = []\n",
    "    target_nodes_local_idx = []\n",
    "\n",
    "    sorted_time_frames = sorted(df_lineage['time_frame'].unique())\n",
    "\n",
    "    for i in range(len(sorted_time_frames) - 1):\n",
    "        current_t = sorted_time_frames[i]\n",
    "        next_t = sorted_time_frames[i+1]\n",
    "\n",
    "        df_current_t = df_lineage[df_lineage['time_frame'] == current_t]\n",
    "        df_next_t = df_lineage[df_lineage['time_frame'] == next_t]\n",
    "\n",
    "        current_lineage_to_node = df_current_t.set_index('ground_truth_lineage')['node_id'].to_dict()\n",
    "        next_lineage_to_node = df_next_t.set_index('ground_truth_lineage')['node_id'].to_dict()\n",
    "\n",
    "        for idx, row in df_current_t.iterrows():\n",
    "            current_global_node_id = row['node_id']\n",
    "            current_ground_truth_lineage = row['ground_truth_lineage']\n",
    "\n",
    "            if current_ground_truth_lineage in next_lineage_to_node:\n",
    "                next_global_node_id = next_lineage_to_node[current_ground_truth_lineage]\n",
    "                source_nodes_local_idx.append(global_id_to_local_idx[current_global_node_id])\n",
    "                target_nodes_local_idx.append(global_id_to_local_idx[next_global_node_id])\n",
    "\n",
    "            daughter1_lineage = f\"{current_ground_truth_lineage}.1\"\n",
    "            daughter2_lineage = f\"{current_ground_truth_lineage}.2\"\n",
    "\n",
    "            if daughter1_lineage in next_lineage_to_node:\n",
    "                next_global_node_id = next_lineage_to_node[daughter1_lineage]\n",
    "                source_nodes_local_idx.append(global_id_to_local_idx[current_global_node_id])\n",
    "                target_nodes_local_idx.append(global_id_to_local_idx[next_global_node_id])\n",
    "            if daughter2_lineage in next_lineage_to_node:\n",
    "                next_global_node_id = next_lineage_to_node[daughter2_lineage]\n",
    "                source_nodes_local_idx.append(global_id_to_local_idx[current_global_node_id])\n",
    "                target_nodes_local_idx.append(global_id_to_local_idx[next_global_node_id])\n",
    "\n",
    "    if not source_nodes_local_idx:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long).to(device)\n",
    "    else:\n",
    "        unique_edges = list(set(zip(source_nodes_local_idx, target_nodes_local_idx)))\n",
    "        source_nodes_unique, target_nodes_unique = zip(*unique_edges)\n",
    "        edge_index = torch.tensor([list(source_nodes_unique), list(target_nodes_unique)], dtype=torch.long).to(device)\n",
    "\n",
    "    data = Data(x=x,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                pos=pos,\n",
    "                num_nodes=num_nodes,\n",
    "                 time_frame=node_time_frames,\n",
    "                root_lineage_branch=df_lineage['ground_truth_lineage'].iloc[0], # The GTL that defines this subgraph\n",
    "                start_time_frame=df_lineage['time_frame'].min(),\n",
    "                experiment_name=df_lineage['experiment_name'].iloc[0],\n",
    "                fov=df_lineage['FOV'].iloc[0],\n",
    "                trench_id=df_lineage['trench_id'].iloc[0]\n",
    "               )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa9d352e-41c5-4504-ba80-87ef92058696",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_lineage_roots_tuples = identify_sub_lineage_roots(df_cells)\n",
    "branch_graphs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47bb955d-a6c5-42a9-a660-ffccaa832808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully created 359 PyG graphs, one per identified sub-lineage branch.\n"
     ]
    }
   ],
   "source": [
    "for exp, fov, trench, root_lineage_str, start_t in sub_lineage_roots_tuples:\n",
    "    # Filter the DataFrame to include only the cells belonging to this specific sub-lineage branch\n",
    "    # and starting from its first appearance time\n",
    "    df_branch = df_cells[\n",
    "        (df_cells['experiment_name'] == exp) &\n",
    "        (df_cells['FOV'] == fov) &\n",
    "        (df_cells['trench_id'] == trench) &\n",
    "        (df_cells['time_frame'] >= start_t) &\n",
    "        # This regex ensures we only get descendants of this specific root_lineage_str\n",
    "        (df_cells['ground_truth_lineage'].apply(lambda x: x == root_lineage_str or x.startswith(f\"{root_lineage_str}.\")))\n",
    "    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Ensure the branch actually has cells, otherwise skip\n",
    "    if not df_branch.empty:\n",
    "        #print(f\"Processing Sub-Lineage: '{root_lineage_str}' starting at t={start_t} (Exp: {exp}, FOV: {fov}, Trench: {trench})...\")\n",
    "        graph = create_lineage_graph(df_branch, device=device)\n",
    "        if graph is not None:\n",
    "            branch_graphs_list.append(graph)\n",
    "        else:\n",
    "            print(f\"  Skipped (no valid connections/nodes) for '{root_lineage_str}' at t={start_t}\")\n",
    "    else:\n",
    "        print(f\"  Skipped (empty DataFrame) for '{root_lineage_str}' at t={start_t}\")\n",
    "\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(branch_graphs_list)} PyG graphs, one per identified sub-lineage branch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb7fefa8-aba1-4142-a2ac-492c6f90f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Sub-Lineage Graph Details:\n",
      "\n",
      "--- Graph 1 ---\n",
      "Data(x=[359, 10], edge_index=[2, 342], y=[359], pos=[359], num_nodes=359, time_frame=[359], root_lineage_branch='A.1.1.2', start_time_frame=0, experiment_name='DUMM_giTG69_Glucose_013025', fov='007', trench_id='295')\n",
      "  Nodes (x.shape): torch.Size([359, 10])\n",
      "  Edges (edge_index.shape): torch.Size([2, 342])\n",
      "  Labels (y.shape): torch.Size([359])\n",
      "  Root Lineage Branch ID: A.1.1.2\n",
      "  Start Time Frame: 0\n",
      "  Experiment Name: DUMM_giTG69_Glucose_013025\n",
      "  FOV: 007\n",
      "  Trench ID: 295\n",
      "\n",
      "--- Graph 2 ---\n",
      "Data(x=[267, 10], edge_index=[2, 259], y=[267], pos=[267], num_nodes=267, time_frame=[267], root_lineage_branch='A.2.1.2.1', start_time_frame=1, experiment_name='DUMM_giTG69_Glucose_013025', fov='007', trench_id='295')\n",
      "  Nodes (x.shape): torch.Size([267, 10])\n",
      "  Edges (edge_index.shape): torch.Size([2, 259])\n",
      "  Labels (y.shape): torch.Size([267])\n",
      "  Root Lineage Branch ID: A.2.1.2.1\n",
      "  Start Time Frame: 1\n",
      "  Experiment Name: DUMM_giTG69_Glucose_013025\n",
      "  FOV: 007\n",
      "  Trench ID: 295\n",
      "\n",
      "--- Graph 3 ---\n",
      "Data(x=[266, 10], edge_index=[2, 258], y=[266], pos=[266], num_nodes=266, time_frame=[266], root_lineage_branch='A.2.1.2.1', start_time_frame=2, experiment_name='DUMM_giTG69_Glucose_013025', fov='007', trench_id='295')\n",
      "  Nodes (x.shape): torch.Size([266, 10])\n",
      "  Edges (edge_index.shape): torch.Size([2, 258])\n",
      "  Labels (y.shape): torch.Size([266])\n",
      "  Root Lineage Branch ID: A.2.1.2.1\n",
      "  Start Time Frame: 2\n",
      "  Experiment Name: DUMM_giTG69_Glucose_013025\n",
      "  FOV: 007\n",
      "  Trench ID: 295\n"
     ]
    }
   ],
   "source": [
    "if branch_graphs_list:\n",
    "    print(\"\\nExample Sub-Lineage Graph Details:\")\n",
    "    for i, graph in enumerate(branch_graphs_list[:3]): # Print details for first 3 graphs\n",
    "        print(f\"\\n--- Graph {i+1} ---\")\n",
    "        print(graph)\n",
    "        print(f\"  Nodes (x.shape): {graph.x.shape}\")\n",
    "        print(f\"  Edges (edge_index.shape): {graph.edge_index.shape}\")\n",
    "        print(f\"  Labels (y.shape): {graph.y.shape}\")\n",
    "        print(f\"  Root Lineage Branch ID: {graph.root_lineage_branch}\")\n",
    "        print(f\"  Start Time Frame: {graph.start_time_frame}\")\n",
    "        print(f\"  Experiment Name: {graph.experiment_name}\")\n",
    "        print(f\"  FOV: {graph.fov}\")\n",
    "        print(f\"  Trench ID: {graph.trench_id}\")\n",
    "        # Optionally, print the actual ground truth lineage IDs in this subgraph for verification\n",
    "        # print(f\"  Included Lineages: {df_cells.loc[graph.node_id.cpu().numpy()]['ground_truth_lineage'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ccbb9c0c-ab76-4b83-b6d7-44c7d09a1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of graphs: 359\n",
      "Number of training graphs: 251\n",
      "Number of validation graphs: 54\n",
      "Number of test graphs: 54\n"
     ]
    }
   ],
   "source": [
    "# Define split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15 \n",
    "\n",
    "# Ensure ratios sum to 1\n",
    "assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n",
    "\n",
    "# Sequentially split data because train_test_split can only split into 2 sets\n",
    "train_graphs, temp_graphs = train_test_split(\n",
    "    branch_graphs_list,\n",
    "    test_size=(val_ratio + test_ratio), # Combined size for validation and test\n",
    "    random_state=0 # set seed for reproducibility\n",
    ")\n",
    "\n",
    "# split temp_graphs into validation and test sets\n",
    "# (test_ratio / (val_ratio + test_ratio)) ensures the correct proportion from the temporary set\n",
    "val_graphs, test_graphs = train_test_split(\n",
    "    temp_graphs,\n",
    "    test_size=(test_ratio / (val_ratio + test_ratio)),\n",
    "    random_state=0 # set seed for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Total number of graphs: {len(branch_graphs_list)}\")\n",
    "print(f\"Number of training graphs: {len(train_graphs)}\")\n",
    "print(f\"Number of validation graphs: {len(val_graphs)}\")\n",
    "print(f\"Number of test graphs: {len(test_graphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f590885b-cf65-414c-9f9f-a11e5a3fccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyG Datasets created:\n",
      "Train dataset size: 251\n",
      "Validation dataset size: 54\n",
      "Test dataset size: 54\n"
     ]
    }
   ],
   "source": [
    "class LineageDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super().__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = LineageDataset(train_graphs)\n",
    "val_dataset = LineageDataset(val_graphs)\n",
    "test_dataset = LineageDataset(test_graphs)\n",
    "\n",
    "print(\"\\nPyG Datasets created:\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85ce68ea-24c4-4896-9062-4633541c8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aac02fe4-2dd4-4835-86ca-fe13bb683cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, negative_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87f5b1-4114-4a43-b0de-2f3372e063ab",
   "metadata": {},
   "source": [
    "class LineageLinkPredictionGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(LineageLinkPredictionGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels) \n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels) # Output node embeddings\n",
    "\n",
    "        # Decoder/multilayer perceptron for link prediction:\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_channels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) # Output a single logit for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # GNN Encoder\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index) # x are now node embeddings\n",
    "\n",
    "        return x # Return node embeddings\n",
    "    \n",
    "    def decode(self, z, pos_edge_index, neg_edge_index=None):\n",
    "        # z: node embeddings from forward()\n",
    "        # pos_edge_index: actual edges in the batch\n",
    "        # neg_edge_index: sampled negative edges in the batch\n",
    "\n",
    "        edge_indices = torch.cat([pos_edge_index, neg_edge_index], dim=-1) if neg_edge_index is not None else pos_edge_index\n",
    "\n",
    "        # Concatenate embeddings of source and target nodes for each edge\n",
    "        source_embed = z[edge_indices[0]]\n",
    "        target_embed = z[edge_indices[1]]\n",
    "        edge_features = torch.cat([source_embed, target_embed], dim=-1) # Concatenate\n",
    "\n",
    "        # Pass through decoder to get logits\n",
    "        logits = self.decoder(edge_features).squeeze(-1) # Squeeze to get [num_edges]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244209f-2c73-473e-99f4-f67ab94f0189",
   "metadata": {},
   "source": [
    "# Model and Optimizer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3d8b2-00d0-48f3-b4e6-5571496c6acf",
   "metadata": {},
   "source": [
    "# determine number of node features in dataset\n",
    "num_node_features = len(node_feature_cols)\n",
    "\n",
    "model = LineageLinkPredictionGNN(in_channels=num_node_features, \n",
    "                                 hidden_channels=64).to(device) # consider less hidden channels to prevent overfitting, maybe 16 or 32, or go way higher like 256 or 512\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss() # For binary classification of edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ede06-86f7-4c42-a935-4a6602a5026a",
   "metadata": {},
   "source": [
    "def generate_local_temporal_negative_samples(data: Data, num_neg_samples_per_pos_edge: float, radius_threshold: float, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates negative samples by considering only cells in consecutive time frames\n",
    "    and within a certain spatial radius of potential source nodes, excluding true positives.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): A single graph batch containing x, edge_index, pos, time_frame.\n",
    "        num_neg_samples_per_pos_edge (float): Ratio of negative samples to positive samples.\n",
    "                                                e.g., 1.0 for 1:1, 2.0 for 2:1.\n",
    "        radius_threshold (float): Maximum spatial distance for a potential negative connection.\n",
    "        device (str): Device to put tensors on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: edge_index of sampled negative connections, shape [2, num_neg_samples].\n",
    "    \"\"\"\n",
    "    if data.edge_index.numel() == 0: # No positive edges, no negative samples possible this way\n",
    "        return torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "\n",
    "    # Convert tensors to CPU for easier numpy/list processing if needed, then back to device\n",
    "    pos_coords = data.pos.cpu().numpy() # Assuming pos is [num_nodes, 2] (y,x) or [num_nodes, 1] (y)\n",
    "    time_frames = data.time_frame.cpu().numpy()\n",
    "    num_nodes = data.num_nodes\n",
    "    existing_edges = set(tuple(e) for e in data.edge_index.cpu().T.tolist()) # Convert to set for fast lookup\n",
    "\n",
    "    potential_neg_samples = []\n",
    "\n",
    "    # Iterate through all possible source nodes\n",
    "    for i in range(num_nodes):\n",
    "        current_node_time = time_frames[i]\n",
    "        current_node_pos = pos_coords[i]\n",
    "\n",
    "        # Iterate through all possible target nodes (j)\n",
    "        for j in range(num_nodes):\n",
    "            # 1. Temporal Constraint: Only consider next time frame\n",
    "            if time_frames[j] != current_node_time + 1:\n",
    "                continue\n",
    "\n",
    "            # 2. Local Constraint: Check spatial proximity (Euclidean distance)\n",
    "            target_node_pos = pos_coords[j]\n",
    "            # Adjust distance calculation based on your 'pos' dimension\n",
    "            if pos_coords.ndim == 1: # If 'pos' is just centroid_y (1D)\n",
    "                distance = np.abs(current_node_pos - target_node_pos)\n",
    "            else: # If 'pos' is (y, x) or (x, y) etc. (2D or more)\n",
    "                distance = np.linalg.norm(current_node_pos - target_node_pos)\n",
    "\n",
    "            if distance > radius_threshold:\n",
    "                continue\n",
    "\n",
    "            # 3. Exclude existing positive edges\n",
    "            if (i, j) not in existing_edges:\n",
    "                potential_neg_samples.append((i, j))\n",
    "\n",
    "    # Convert to tensor\n",
    "    if not potential_neg_samples:\n",
    "        return torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "\n",
    "    potential_neg_samples_tensor = torch.tensor(potential_neg_samples, dtype=torch.long).T.to(device)\n",
    "\n",
    "    # Sample a subset of potential negative samples based on ratio\n",
    "    num_positive_edges = data.edge_index.size(1)\n",
    "    desired_neg_samples = int(num_positive_edges * num_neg_samples_per_pos_edge)\n",
    "\n",
    "    if desired_neg_samples >= potential_neg_samples_tensor.size(1):\n",
    "        # If not enough potential negatives, take all of them\n",
    "        return potential_neg_samples_tensor\n",
    "    else:\n",
    "        # Randomly sample the desired number of negative edges\n",
    "        indices = torch.randperm(potential_neg_samples_tensor.size(1), device=device)[:desired_neg_samples]\n",
    "        return potential_neg_samples_tensor[:, indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5c986-a00d-42c6-8870-09100661d2f8",
   "metadata": {},
   "source": [
    "def train_link_prediction(model, train_loader, optimizer, criterion, device, neg_sample_ratio=1.0, radius_threshold=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Generate negative edges\n",
    "        neg_edge_index = generate_local_temporal_negative_samples(\n",
    "            data,\n",
    "            num_neg_samples_per_pos_edge=neg_sample_ratio, # How many neg samples per positive edge\n",
    "            radius_threshold=radius_threshold, # Spatial threshold (e.g., 50.0 units)\n",
    "            device=device\n",
    "        )\n",
    "        if neg_edge_index.numel() == 0: # Handle cases where no negative samples could be generated\n",
    "            # Skip this batch or generate purely random ones if desired\n",
    "            print(\"Warning: No negative samples generated for a batch. Skipping or using random.\")\n",
    "            # Fallback to random if no local negatives are found in rare cases\n",
    "            neg_edge_index = torch_geometric.utils.negative_sampling(\n",
    "                data.edge_index, num_nodes=data.num_nodes, num_neg_samples=data.edge_index.size(1)).to(device)\n",
    "\n",
    "\n",
    "        # 2. Get node embeddings from GNN encoder\n",
    "        z = model(data.x, data.edge_index)\n",
    "\n",
    "        # 3. Decode edges (both positive and negative)\n",
    "        pos_logits = model.decode(z, data.edge_index) # Logits for true edges\n",
    "        neg_logits = model.decode(z, neg_edge_index) # Logits for sampled negative edges\n",
    "\n",
    "        # 4. Create labels: 1 for positive edges, 0 for negative edges\n",
    "        pos_labels = torch.ones(pos_logits.size(0), device=device)\n",
    "        neg_labels = torch.zeros(neg_logits.size(0), device=device)\n",
    "\n",
    "        # 5. Concatenate logits and labels\n",
    "        logits = torch.cat([pos_logits, neg_logits])\n",
    "        labels = torch.cat([pos_labels, neg_labels])\n",
    "\n",
    "        # 6. Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_link_prediction(model, loader, criterion, device, neg_sample_ratio=1.0, radius_threshold=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Generate negative edges for evaluation\n",
    "            neg_edge_index = generate_local_temporal_negative_samples(\n",
    "                data,\n",
    "                num_neg_samples_per_pos_edge=neg_sample_ratio,\n",
    "                radius_threshold=radius_threshold,\n",
    "                device=device\n",
    "            )\n",
    "            if neg_edge_index.numel() == 0:\n",
    "                # If no local negatives, use random for evaluation as a fallback\n",
    "                neg_edge_index = torch_geometric.utils.negative_sampling(\n",
    "                    data.edge_index, num_nodes=data.num_nodes, num_neg_samples=data.edge_index.size(1)).to(device)\n",
    "\n",
    "\n",
    "            z = model(data.x, data.edge_index)\n",
    "            pos_logits = model.decode(z, data.edge_index)\n",
    "            neg_logits = model.decode(z, neg_edge_index)\n",
    "\n",
    "            pos_labels = torch.ones(pos_logits.size(0), device=device)\n",
    "            neg_labels = torch.zeros(neg_logits.size(0), device=device)\n",
    "\n",
    "            logits = torch.cat([pos_logits, neg_logits])\n",
    "            labels = torch.cat([pos_labels, neg_labels])\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long() # Convert logits to binary predictions\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    accuracy = (all_preds == all_labels).float().mean().item()\n",
    "    # You might also want to calculate precision, recall, F1-score for link prediction\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d438a6-15c9-4135-a88d-af8c0534e462",
   "metadata": {},
   "source": [
    "# My LineageLinkPredictionGNN is not learning anything, going to try changing architecture like https://arxiv.org/pdf/2202.04731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a016c0c8-0d26-48ce-b274-964e30de5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree # For potentially normalizing attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1df39ed5-2d0d-4c1f-ab68-039da98e3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function expects a sub-DataFrame already filtered for a specific lineage branch\n",
    "def create_lineage_graph(df_lineage, device='cpu'):\n",
    "    original_global_node_ids = df_lineage['node_id'].values\n",
    "    global_id_to_local_idx = {global_id: i for i, global_id in enumerate(original_global_node_ids)}\n",
    "\n",
    "    x = torch.tensor(df_lineage[node_feature_cols].values, dtype=torch.float).to(device)\n",
    "    y = torch.tensor(df_lineage['numeric_lineage'].values, dtype=torch.long).to(device)\n",
    "    # Ensure pos is 2D if you have both x and y, or handle 1D appropriately in negative sampling\n",
    "    # If your 'pos' is just centroid_y, then it's [num_nodes]. Need to reshape to [num_nodes, 1]\n",
    "    # for consistent tensor operations.\n",
    "    pos = torch.tensor(df_lineage['centroid_y'].values, dtype=torch.float).to(device)\n",
    "    if pos.dim() == 1: # Reshape if it's just a 1D tensor of y-coords\n",
    "        pos = pos.unsqueeze(1) # Makes it [num_nodes, 1]\n",
    "\n",
    "    node_time_frames = torch.tensor(df_lineage['time_frame'].values, dtype=torch.long).to(device)\n",
    "\n",
    "    num_nodes = len(df_lineage)\n",
    "    if num_nodes == 0:\n",
    "        return None\n",
    "\n",
    "    source_nodes_local_idx = []\n",
    "    target_nodes_local_idx = []\n",
    "\n",
    "    sorted_time_frames = sorted(df_lineage['time_frame'].unique())\n",
    "\n",
    "    # This loop builds the lists of source_nodes_local_idx and target_nodes_local_idx\n",
    "    for i in range(len(sorted_time_frames) - 1):\n",
    "        current_t = sorted_time_frames[i]\n",
    "        next_t = sorted_time_frames[i+1]\n",
    "\n",
    "        df_current_t = df_lineage[df_lineage['time_frame'] == current_t]\n",
    "        df_next_t = df_lineage[df_lineage['time_frame'] == next_t]\n",
    "\n",
    "        current_lineage_to_node = df_current_t.set_index('ground_truth_lineage')['node_id'].to_dict()\n",
    "        next_lineage_to_node = df_next_t.set_index('ground_truth_lineage')['node_id'].to_dict()\n",
    "\n",
    "        for idx, row in df_current_t.iterrows():\n",
    "            current_global_node_id = row['node_id']\n",
    "            current_ground_truth_lineage = row['ground_truth_lineage']\n",
    "\n",
    "            if current_ground_truth_lineage in next_lineage_to_node:\n",
    "                next_global_node_id = next_lineage_to_node[current_ground_truth_lineage]\n",
    "                source_nodes_local_idx.append(global_id_to_local_idx[current_global_node_id])\n",
    "                target_nodes_local_idx.append(global_id_to_local_idx[next_global_node_id])\n",
    "\n",
    "            daughter1_lineage = f\"{current_ground_truth_lineage}.1\"\n",
    "            daughter2_lineage = f\"{current_ground_truth_lineage}.2\"\n",
    "\n",
    "            if daughter1_lineage in next_lineage_to_node:\n",
    "                next_global_node_id = next_lineage_to_node[daughter1_lineage]\n",
    "                source_nodes_local_idx.append(global_id_to_local_idx[current_global_node_id])\n",
    "                target_nodes_local_idx.append(global_id_to_local_idx[next_global_node_id])\n",
    "            if daughter2_lineage in next_lineage_to_node:\n",
    "                next_global_node_id = next_lineage_to_node[daughter2_lineage]\n",
    "                source_nodes_local_idx.append(global_id_to_local_idx[current_global_node_id])\n",
    "                target_nodes_local_idx.append(global_id_to_local_idx[next_global_node_id])\n",
    "\n",
    "    # --- Determine edge_index based on collected source/target lists ---\n",
    "    if not source_nodes_local_idx: # Case: No edges found at all for this lineage branch\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long).to(device)\n",
    "        # For this case, initial_edge_attr must also be an empty tensor\n",
    "        initial_edge_attr = torch.empty((0, len(node_feature_cols) + 1), dtype=torch.float).to(device)\n",
    "    else: # Case: Some potential edges were found\n",
    "        unique_edges = list(set(zip(source_nodes_local_idx, target_nodes_local_idx)))\n",
    "        if not unique_edges: # This should theoretically be covered by the first 'if', but acts as a safeguard\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long).to(device)\n",
    "            initial_edge_attr = torch.empty((0, len(node_feature_cols) + 1), dtype=torch.float).to(device)\n",
    "        else: # Case: Non-empty, unique edges exist\n",
    "            source_nodes_unique, target_nodes_unique = zip(*unique_edges)\n",
    "            edge_index = torch.tensor([list(source_nodes_unique), list(target_nodes_unique)], dtype=torch.long).to(device)\n",
    "\n",
    "            # --- ONLY in this 'else' block, calculate initial_edge_attr for the actual edges ---\n",
    "            # Get node features for source and target nodes of the created edges\n",
    "            # (Use df_lineage.iloc with the numpy conversion of edge_index for correct indexing)\n",
    "            true_src_node_dfs = df_lineage.iloc[edge_index[0].cpu().numpy()]\n",
    "            true_tgt_node_dfs = df_lineage.iloc[edge_index[1].cpu().numpy()]\n",
    "\n",
    "            initial_edge_features_list = []\n",
    "            for i in range(edge_index.size(1)): # Iterate over the actual unique edges\n",
    "                # Use original node features for initial D-S block\n",
    "                v_i_raw = torch.tensor(true_src_node_dfs.iloc[i][node_feature_cols].values, dtype=torch.float)\n",
    "                v_j_raw = torch.tensor(true_tgt_node_dfs.iloc[i][node_feature_cols].values, dtype=torch.float)\n",
    "                initial_edge_features_list.append(DS_block(v_i_raw, v_j_raw))\n",
    "\n",
    "            initial_edge_attr = torch.stack(initial_edge_features_list, dim=0).to(device)\n",
    "            # No 'else' needed here, as this block only runs if initial_edge_features_list is guaranteed to be non-empty\n",
    "            # because edge_index.size(1) > 0.\n",
    "\n",
    "    data = Data(x=x,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                pos=pos,\n",
    "                num_nodes=num_nodes,\n",
    "                time_frame=node_time_frames,\n",
    "                edge_attr=initial_edge_attr, # This will now always be a correctly shaped tensor\n",
    "                root_lineage_branch=df_lineage['ground_truth_lineage'].iloc[0],\n",
    "                start_time_frame=df_lineage['time_frame'].min(),\n",
    "                experiment_name=df_lineage['experiment_name'].iloc[0],\n",
    "                fov=df_lineage['FOV'].iloc[0],\n",
    "                trench_id=df_lineage['trench_id'].iloc[0]\n",
    "               )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f725cce-6d1a-4580-b9ae-80bfa4eadf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_local_temporal_negative_samples(data: Data, num_neg_samples_per_pos_edge: float, radius_threshold: float, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates negative samples by considering only cells in consecutive time frames\n",
    "    and within a certain spatial radius of potential source nodes, excluding true positives.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): A single graph batch containing x, edge_index, pos, time_frame.\n",
    "        num_neg_samples_per_pos_edge (float): Ratio of negative samples to positive samples.\n",
    "                                                e.g., 1.0 for 1:1, 2.0 for 2:1.\n",
    "        radius_threshold (float): Maximum spatial distance for a potential negative connection.\n",
    "        device (str): Device to put tensors on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: edge_index of sampled negative connections, shape [2, num_neg_samples].\n",
    "    \"\"\"\n",
    "    if data.edge_index.numel() == 0: # No positive edges, no negative samples possible this way\n",
    "        return torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "\n",
    "    # Convert tensors to CPU for easier numpy/list processing if needed, then back to device\n",
    "    pos_coords = data.pos.cpu().numpy() # Assuming pos is [num_nodes, 2] (y,x) or [num_nodes, 1] (y)\n",
    "    time_frames = data.time_frame.cpu().numpy()\n",
    "    num_nodes = data.num_nodes\n",
    "    existing_edges = set(tuple(e) for e in data.edge_index.cpu().T.tolist()) # Convert to set for fast lookup\n",
    "\n",
    "    potential_neg_samples = []\n",
    "\n",
    "    # Iterate through all possible source nodes\n",
    "    for i in range(num_nodes):\n",
    "        current_node_time = time_frames[i]\n",
    "        current_node_pos = pos_coords[i]\n",
    "\n",
    "        # Iterate through all possible target nodes (j)\n",
    "        for j in range(num_nodes):\n",
    "            # 1. Temporal Constraint: Only consider next time frame\n",
    "            if time_frames[j] != current_node_time + 1:\n",
    "                continue\n",
    "\n",
    "            # 2. Local Constraint: Check spatial proximity (Euclidean distance)\n",
    "            target_node_pos = pos_coords[j]\n",
    "            # Adjust distance calculation based on your 'pos' dimension\n",
    "            if pos_coords.ndim == 1: # If 'pos' is just centroid_y (1D)\n",
    "                distance = np.abs(current_node_pos - target_node_pos)\n",
    "            else: # If 'pos' is (y, x) or (x, y) etc. (2D or more)\n",
    "                distance = np.linalg.norm(current_node_pos - target_node_pos)\n",
    "\n",
    "            if distance > radius_threshold:\n",
    "                continue\n",
    "\n",
    "            # 3. Exclude existing positive edges\n",
    "            if (i, j) not in existing_edges:\n",
    "                potential_neg_samples.append((i, j))\n",
    "\n",
    "    # Convert to tensor\n",
    "    if not potential_neg_samples:\n",
    "        return torch.empty((2, 0), dtype=torch.long, device=device), torch.empty((0, data.x.size(1) + 1), dtype=torch.float, device=device)\n",
    "\n",
    "    potential_neg_samples_tensor = torch.tensor(potential_neg_samples, dtype=torch.long).T.to(device)\n",
    "\n",
    "    # Sample a subset\n",
    "    num_positive_edges = data.edge_index.size(1)\n",
    "    desired_neg_samples = int(num_positive_edges * num_neg_samples_per_pos_edge)\n",
    "\n",
    "    if desired_neg_samples >= potential_neg_samples_tensor.size(1):\n",
    "        sampled_neg_edge_index = potential_neg_samples_tensor\n",
    "    else:\n",
    "        indices = torch.randperm(potential_neg_samples_tensor.size(1), device=device)[:desired_neg_samples]\n",
    "        sampled_neg_edge_index = potential_neg_samples_tensor[:, indices]\n",
    "\n",
    "    # Compute initial edge_attr for the sampled negative edges\n",
    "    neg_src_nodes_indices = sampled_neg_edge_index[0]\n",
    "    neg_tgt_nodes_indices = sampled_neg_edge_index[1]\n",
    "\n",
    "    initial_neg_edge_attr_list = []\n",
    "    for i in range(sampled_neg_edge_index.size(1)):\n",
    "        v_i_raw = data.x[neg_src_nodes_indices[i]] # Use original node features (data.x)\n",
    "        v_j_raw = data.x[neg_tgt_nodes_indices[i]]\n",
    "        initial_neg_edge_attr_list.append(DS_block(v_i_raw, v_j_raw))\n",
    "\n",
    "    if initial_neg_edge_attr_list:\n",
    "        initial_neg_edge_attr = torch.stack(initial_neg_edge_attr_list, dim=0).to(device)\n",
    "    else:\n",
    "        initial_neg_edge_attr = torch.empty((0, data.x.size(1) + 1), dtype=torch.float).to(device)\n",
    "\n",
    "    return sampled_neg_edge_index, initial_neg_edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd4464-5b5d-487d-9da0-db8dc2c3641a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db12a347-573a-409b-b561-5141d7c69dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper MLP for f_PDN_edge (Attention Weights) ---\n",
    "class PDNEdgeMLP(nn.Module):\n",
    "    def __init__(self, edge_feature_dim, out_dim=1):\n",
    "        super().__init__()\n",
    "        # Simplified MLP for attention weights (scalar output)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(edge_feature_dim, 32), # Example hidden dim\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, out_dim)\n",
    "        )\n",
    "    def forward(self, z): # z is edge feature\n",
    "        return self.mlp(z)\n",
    "\n",
    "# --- Helper MLP for f_PDN_node (Node Feature Transformation) ---\n",
    "class PDNNodeMLP(nn.Module):\n",
    "    def __init__(self, node_feature_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # MLP for transforming node features before aggregation\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_feature_dim, out_dim), # Typically out_dim = node_feature_dim\n",
    "            nn.ReLU()\n",
    "            # No final ReLU if you want negative values for weighted sum, or add Batch Norm\n",
    "        )\n",
    "    def forward(self, x): # x is node feature\n",
    "        return self.mlp(x)\n",
    "\n",
    "# --- D-S Block (Distance & Similarity) ---\n",
    "def DS_block(v_i, v_j):\n",
    "    \"\"\"\n",
    "    Calculates Distance & Similarity vector for two node feature vectors.\n",
    "    Equivalent to Eq. 3 in the paper.\n",
    "    Args:\n",
    "        v_i (torch.Tensor): Feature vector of node i, shape [d_v].\n",
    "        v_j (torch.Tensor): Feature vector of node j, shape [d_v].\n",
    "    Returns:\n",
    "        torch.Tensor: Concatenated vector of absolute differences and cosine similarity, shape [d_v + 1].\n",
    "    \"\"\"\n",
    "    abs_diff = torch.abs(v_i - v_j)\n",
    "    cosine_similarity = F.cosine_similarity(v_i.unsqueeze(0), v_j.unsqueeze(0)).squeeze(0)\n",
    "    return torch.cat([abs_diff, cosine_similarity.unsqueeze(0)], dim=-1) # Unsqueeze for scalar cos_sim\n",
    "\n",
    "# --- The EP-MPNN Block ---\n",
    "class EP_MPNN_Block(MessagePassing):\n",
    "    def __init__(self, node_channels, edge_channels):\n",
    "        super().__init__(aggr='add', flow='source_to_target') # Aggregation for node update. source_to_target for N(i) being t-1 nodes.\n",
    "        self.node_channels = node_channels\n",
    "        self.edge_channels = edge_channels\n",
    "\n",
    "        # Node Feature Update components (PDN-Conv)\n",
    "        # f_PDN_edge: Maps edge features to scalar attention weights (omega)\n",
    "        self.f_pdn_edge = PDNEdgeMLP(edge_channels, out_dim=1)\n",
    "        # f_PDN_node: Transforms node features (tilde_x)\n",
    "        self.f_pdn_node = PDNNodeMLP(node_channels, node_channels) # Output dim same as input for residuals\n",
    "\n",
    "        # Edge Feature Update components (Edge Encoder)\n",
    "        # f_EE_edge: MLP to update edge features.\n",
    "        # Input: current edge_features (edge_channels)\n",
    "        #        + updated node_features from source (node_channels)\n",
    "        #        + updated node_features from target (node_channels)\n",
    "        #        + D-S block output (node_channels + 1)\n",
    "        self.f_ee_edge = nn.Sequential(\n",
    "            nn.Linear(edge_channels + 2 * node_channels + (node_channels + 1), 128), # Example hidden size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, edge_channels) # Output dim same as edge_channels\n",
    "        )\n",
    "\n",
    "        # BatchNorm (optional but often helpful for stability)\n",
    "        self.bn_node = nn.BatchNorm1d(node_channels)\n",
    "        self.bn_edge = nn.BatchNorm1d(edge_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: node features X^(l-1)\n",
    "        # edge_index: graph connectivity\n",
    "        # edge_attr: edge features Z^(l-1)\n",
    "\n",
    "        # 1. Edge Feature Update (first for this block, as per paper's description \"In the l-th block vi = x(l)i and vj = x(l)j\")\n",
    "        # However, the paper implies x(l) is used. Let's assume for simplicity first block\n",
    "        # uses x(l-1) and subsequent blocks use x(l).\n",
    "        # To align with: \"In the l-th block vi = x(l)i and vj = x(l)j.\" and \"the features of an edge ej,i are updated based on the features of i and j\"\n",
    "        # This means edge update uses nodes *after* they are potentially updated by previous block.\n",
    "        # For l=0 (initial), x(0) are raw features. For l>0, x(l) comes from PDN-Conv.\n",
    "        # For simplicity, let's make it work sequentially: update nodes, THEN update edges using new nodes.\n",
    "        # Or, as paper implies \"alternately updated\", meaning within the block loop:\n",
    "        # Step A: Compute updated nodes x^(l) from x^(l-1) and z^(l-1)\n",
    "        # Step B: Compute updated edges z^(l) from x^(l) and z^(l-1)\n",
    "        # Let's follow this:\n",
    "\n",
    "        # Cache inputs for edge update after node update\n",
    "        x_prev = x\n",
    "        edge_attr_prev = edge_attr\n",
    "\n",
    "        # 2. Node Feature Update (PDN-Conv: Eq. 2)\n",
    "        # Message passing step\n",
    "        x_updated = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=(x.size(0), x.size(0)))\n",
    "\n",
    "        # Add residual connection and apply BatchNorm/ReLU\n",
    "        x = self.bn_node(x_prev + x_updated) # Residual (assuming input and output dims are same)\n",
    "        x = F.relu(x) # x is now X^(l)\n",
    "\n",
    "        # 3. Edge Feature Update (Edge Encoder: based on x^(l) and z^(l-1))\n",
    "        # Get source and target node embeddings for edges\n",
    "        row, col = edge_index\n",
    "        src_node_features = x[row] # x^(l) for source nodes\n",
    "        tgt_node_features = x[col] # x^(l) for target nodes\n",
    "\n",
    "        # Compute D-S block output for each edge\n",
    "        ds_outputs = []\n",
    "        for i in range(edge_attr.size(0)): # Iterate per edge\n",
    "            ds_outputs.append(DS_block(src_node_features[i], tgt_node_features[i]))\n",
    "        ds_outputs_tensor = torch.stack(ds_outputs, dim=0) # Shape: [num_edges, node_channels + 1]\n",
    "\n",
    "        # Concatenate inputs for f_EE_edge\n",
    "        # current edge_features (Z^(l-1))\n",
    "        # updated node_features from source (X^(l))\n",
    "        # updated node_features from target (X^(l))\n",
    "        # D-S block output (from X^(l), X^(l))\n",
    "        edge_input_for_mlp = torch.cat([\n",
    "            edge_attr_prev, # Z^(l-1)\n",
    "            src_node_features, # X^(l)\n",
    "            tgt_node_features, # X^(l)\n",
    "            ds_outputs_tensor # D-S block on X^(l)\n",
    "        ], dim=-1)\n",
    "\n",
    "        # Pass through edge encoder MLP\n",
    "        edge_attr = self.f_ee_edge(edge_input_for_mlp) # Z^(l)\n",
    "        edge_attr = self.bn_edge(edge_attr) # BatchNorm\n",
    "        edge_attr = F.relu(edge_attr) # ReLU\n",
    "\n",
    "        return x, edge_attr # Return updated nodes (X^(l)) and updated edges (Z^(l))\n",
    "\n",
    "    def message(self, x_j, edge_attr_i): # x_j is neighbor features, edge_attr_i is edge features to neighbor\n",
    "        # x_j: x^(l-1)_j (features of neighbor j)\n",
    "        # edge_attr_i: z^(l-1)_ji (features of edge (j,i))\n",
    "        # Compute omega_ji = f_PDN_edge(z_ji) (attention weight for edge j,i)\n",
    "        omega_ji = self.f_pdn_edge(edge_attr_i)\n",
    "        # Compute tilde_x_j = f_PDN_node(x_j) (mapped feature vector of node j)\n",
    "        tilde_x_j = self.f_pdn_node(x_j)\n",
    "\n",
    "        # The message is omega_ji * tilde_x_j\n",
    "        return omega_ji * tilde_x_j\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "        # inputs: [num_messages, hidden_channels] (omega_ji * tilde_x_j for each edge)\n",
    "        # index: target node index for each message\n",
    "        # dim_size: total number of nodes\n",
    "        # Summation aggregation (as per Eq. 2)\n",
    "        out = super().aggregate(inputs, index, dim_size=dim_size)\n",
    "        return out\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # This is where the output of aggregation (sum_j omega_ji * tilde_x_j)\n",
    "        # is combined with the current node feature.\n",
    "        # But per Eq. 2, the residual is handled in the forward pass.\n",
    "        # So we just return the aggregated messages here.\n",
    "        return aggr_out # This will be the x_updated in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1fdbd338-512e-4c92-8d56-80c74f9994a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineageLinkPredictionGNN(nn.Module):\n",
    "    def __init__(self, in_channels, initial_edge_channels, hidden_channels, num_blocks=2):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        # Initial Linear layer to project input features to hidden_channels\n",
    "        self.initial_node_proj = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # Initial Edge Feature Projector (optional, if initial_edge_channels is different from hidden_channels)\n",
    "        # Or you can define specific initial edge features.\n",
    "        self.initial_edge_proj = nn.Linear(initial_edge_channels, hidden_channels)\n",
    "\n",
    "        # Stack L EP-MPNN blocks\n",
    "        self.ep_mpnn_blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.ep_mpnn_blocks.append(EP_MPNN_Block(hidden_channels, hidden_channels))\n",
    "\n",
    "        # Decoder for link prediction (takes concatenated node embeddings)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_channels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) # Output a single logit for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr # Data now also has edge_attr\n",
    "\n",
    "        # Initial projection of node features\n",
    "        x = F.relu(self.initial_node_proj(x))\n",
    "\n",
    "        # Initial projection of edge features\n",
    "        edge_attr = F.relu(self.initial_edge_proj(edge_attr)) # Ensure initial_edge_channels maps to hidden_channels\n",
    "\n",
    "        # Pass through L EP-MPNN blocks\n",
    "        for block in self.ep_mpnn_blocks:\n",
    "            x, edge_attr = block(x, edge_index, edge_attr) # Both nodes and edges get updated\n",
    "\n",
    "        # x are the final node embeddings after L blocks\n",
    "        return x # Return node embeddings for the decoder\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index=None):\n",
    "        # This decode method remains largely the same as before,\n",
    "        # as it operates on the final node embeddings 'z'.\n",
    "        edge_indices = torch.cat([pos_edge_index, neg_edge_index], dim=-1) if neg_edge_index is not None else pos_edge_index\n",
    "\n",
    "        src_embed = z[edge_indices[0]]\n",
    "        tgt_embed = z[edge_indices[1]]\n",
    "        edge_features = torch.cat([src_embed, tgt_embed], dim=-1) # Concatenate\n",
    "\n",
    "        logits = self.decoder(edge_features).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f61ec77c-ae01-4479-9bf5-9698045aad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link_prediction(model, train_loader, optimizer, criterion, device, neg_sample_ratio=1.0, radius_threshold=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        neg_edge_index, neg_edge_attr_initial = generate_local_temporal_negative_samples(\n",
    "            data,\n",
    "            num_neg_samples_per_pos_edge=neg_sample_ratio,\n",
    "            radius_threshold=radius_threshold,\n",
    "            device=device\n",
    "        )\n",
    "        if neg_edge_index.numel() == 0:\n",
    "            print(\"Warning: No negative samples generated for a batch. Skipping.\")\n",
    "            continue # Skip this batch if no valid negative samples\n",
    "\n",
    "        z = model(data) # Forward pass returns final node embeddings\n",
    "\n",
    "        pos_logits = model.decode(z, data.edge_index)\n",
    "        neg_logits = model.decode(z, neg_edge_index)\n",
    "\n",
    "        pos_labels = torch.ones(pos_logits.size(0), device=device)\n",
    "        neg_labels = torch.zeros(neg_logits.size(0), device=device)\n",
    "\n",
    "        logits = torch.cat([pos_logits, neg_logits])\n",
    "        labels = torch.cat([pos_labels, neg_labels])\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_link_prediction(model, loader, criterion, device, neg_sample_ratio=1.0, radius_threshold=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Generate negative edges for evaluation\n",
    "            neg_edge_index, neg_edge_attr_initial = generate_local_temporal_negative_samples(\n",
    "                data,\n",
    "                num_neg_samples_per_pos_edge=neg_sample_ratio,\n",
    "                radius_threshold=radius_threshold,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            if neg_edge_index.numel() == 0:\n",
    "                print(\"Warning: No negative samples generated for a batch during evaluation. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Combine positive and negative edges for the GNN's forward pass\n",
    "            combined_edge_index = torch.cat([data.edge_index, neg_edge_index], dim=1)\n",
    "            combined_edge_attr_initial = torch.cat([data.edge_attr, neg_edge_attr_initial], dim=0)\n",
    "\n",
    "            temp_data_for_forward = data.clone()\n",
    "            temp_data_for_forward.edge_index = combined_edge_index\n",
    "            temp_data_for_forward.edge_attr = combined_edge_attr_initial\n",
    "\n",
    "            z = model(temp_data_for_forward) # Pass the combined data for message passing\n",
    "\n",
    "            pos_logits = model.decode(z, data.edge_index)\n",
    "            neg_logits = model.decode(z, neg_edge_index)\n",
    "\n",
    "            pos_labels = torch.ones(pos_logits.size(0), device=device)\n",
    "            neg_labels = torch.zeros(neg_logits.size(0), device=device)\n",
    "\n",
    "            logits = torch.cat([pos_logits, neg_logits])\n",
    "            labels = torch.cat([pos_labels, neg_labels])\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long() # Convert logits to binary predictions\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    accuracy = (all_preds == all_labels).float().mean().item()\n",
    "    # You might also want to calculate precision, recall, F1-score for link prediction\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5510c562-ee4b-4bde-a83a-32020e537a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of node features in dataset\n",
    "num_node_features = len(node_feature_cols)\n",
    "\n",
    "# Determine the dimensionality of the initial edge features\n",
    "# As per your DS_block, it's len(node_feature_cols) + 1 (for cosine similarity)\n",
    "initial_edge_feature_dim = len(node_feature_cols) + 1\n",
    "\n",
    "# Instantiate the model with the new parameters\n",
    "\n",
    "model = LineageLinkPredictionGNN(\n",
    "    in_channels=num_node_features,\n",
    "    initial_edge_channels=initial_edge_feature_dim, # <-- NEW REQUIRED ARGUMENT\n",
    "    hidden_channels=64, # Still a hyperparameter to tune (e.g., 16, 32, 128, 256)\n",
    "    num_blocks=2 # <-- NEW OPTIONAL ARGUMENT, paper suggests L blocks\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss() # For binary classification of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d6f647a-5c25-4e3f-95d9-5da0c65aee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting link prediction training with improved negative sampling...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting link prediction training with improved negative sampling...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     train_loss = \u001b[43mtrain_link_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sample_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmy_radius_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     val_loss, val_acc = evaluate_link_prediction(model, val_loader, criterion, device, neg_sample_ratio=\u001b[32m1.0\u001b[39m, radius_threshold=my_radius_threshold)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mVal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_link_prediction\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion, device, neg_sample_ratio, radius_threshold)\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: No negative samples generated for a batch. Skipping.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip this batch if no valid negative samples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m z = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Forward pass returns final node embeddings\u001b[39;00m\n\u001b[32m     21\u001b[39m pos_logits = model.decode(z, data.edge_index)\n\u001b[32m     22\u001b[39m neg_logits = model.decode(z, neg_edge_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.8/envs/pytorch_gnn_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.8/envs/pytorch_gnn_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mLineageLinkPredictionGNN.forward\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     30\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.initial_node_proj(x))\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Initial projection of edge features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m edge_attr = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitial_edge_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# Ensure initial_edge_channels maps to hidden_channels\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Pass through L EP-MPNN blocks\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_mpnn_blocks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.8/envs/pytorch_gnn_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.8/envs/pytorch_gnn_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.8/envs/pytorch_gnn_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# Define your radius_threshold based on your data's spatial scale\n",
    "my_radius_threshold = 50.0 # Example value, adjust this for your data!\n",
    "\n",
    "print(\"\\nStarting link prediction training with improved negative sampling...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train_link_prediction(model, train_loader, optimizer, criterion, device, neg_sample_ratio=1.0, radius_threshold=my_radius_threshold)\n",
    "    val_loss, val_acc = evaluate_link_prediction(model, val_loader, criterion, device, neg_sample_ratio=1.0, radius_threshold=my_radius_threshold)\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, '\n",
    "          f'Train Loss: {train_loss:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# --- Final Evaluation on Test Set ---\n",
    "print(\"\\nEvaluating on test set with improved negative sampling...\")\n",
    "test_loss, test_acc = evaluate_link_prediction(model, test_loader, criterion, device, neg_sample_ratio=1.0, radius_threshold=my_radius_threshold)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dcc4c-afb1-4fc1-b255-792a76f18b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
